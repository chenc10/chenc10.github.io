<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Chen Chen </title> <meta name="author" content="Chen Chen"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://chenc10.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Chen</span> Chen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/team/">Team </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">TACO</abbr> </div> <div id="pengyu2025taming" class="col-sm-8"> <div class="title">Taming Flexible Job Packing in Deep Learning Training Clusters</div> <div class="author"> Pengyu Yang, Weihao Cui, Chunyu Xue, Han Zhao, <em>Chen Chen</em>, Quan Chen, Jing Yang, and Minyi Guo </div> <div class="periodical"> <em>ACM Transactions on Architecture and Code Optimization</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/2025_taco_gimbal.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IPDPS</abbr> </div> <div id="guangqiang2025reducing" class="col-sm-8"> <div class="title">Reducing the End-to-End Latency of DNN-based Recommendation Systems Deployed in GPU Pools</div> <div class="author"> Luan Guangqiang, Pang Pu, Chen Quan, Xu Guoyao, Zhang Chi, Zi Yanyi, Yu Yinghao, Yang Guodong, Zhang Liping, <em>Chen Chen</em>, and Guo Minyi </div> <div class="periodical"> <em>In 39th IEEE International Parallel and Distributed Processing Symposium</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/2025_ipdps_zero.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ENLSP</abbr> </div> <div id="di2024retrieval" class="col-sm-8"> <div class="title">RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval</div> <div class="author"> Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, <em>Chen Chen</em>, Fan Yang, Yuqing Yang, and Lili Qiu </div> <div class="periodical"> <em>In the 4th NeurIPS Workshop on Efficient Natural Language and Speech Processing (Spotlight)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button"><img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> Best Paper Award</a> <a href="/assets/pdf/2024_RetrievalAttention.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Awarded the best paper by <a href="https://neurips2024-enlsp.github.io/index.html" rel="external nofollow noopener" target="_blank">ENLSP</a></p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICPP</abbr> </div> <div id="lv2024fedca" class="col-sm-8"> <div class="title">FedCA: Efficient Federated Learning with Client Autonomy</div> <div class="author"> Na Lv, Zhi Shen, <em>Chen Chen</em>, Zhifeng Jiang, Jiayi Zhang, Quan Chen, and Minyi Guo </div> <div class="periodical"> <em>In Proceedings of the 53rd ACM International Conference on Parallel Processing</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/2024_icpp_fedca.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IWQoS</abbr> </div> <div id="zuo2024pas" class="col-sm-8"> <div class="title">PAS: Towards Accurate and Efficient Federated Learning with Parameter-Adaptive Synchronization</div> <div class="author"> Zuo Gan, <em>Chen Chen</em>, Jiayi Zhang, Gaoxiong Zeng, Yifei Zhu, Jieru Zhao, Quan Chen, and Minyi Guo </div> <div class="periodical"> <em>In Proceedings of the IEEE/ACM International Symposium on Quality of Service</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/2024_iwqos_pas.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IWQoS Poster</abbr> </div> <div id="zuo2024pat" class="col-sm-8"> <div class="title">Towards Efficient Compound Large Language Model System Serving in the Wild</div> <div class="author"> Yifei Zhu, Botao Zhu, <em>Chen Chen</em>, and Xiaoyi Fan </div> <div class="periodical"> <em>In Proceedings of the IEEE/ACM International Symposium on Quality of Service</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button"><img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> Best Poster Award</a> <a href="/assets/pdf/2024_iwqos_pstcs.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p><a href="https://iwqos2024.ieee-iwqos.org/program/award" rel="external nofollow noopener" target="_blank">The best poster for IWQoS 2024</a></p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">OSDI</abbr> </div> <div id="lin_parrot_2024" class="col-sm-8"> <div class="title">Parrot: Efficient Serving of LLM-based Applications with Semantic Variable</div> <div class="author"> Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, <em>Chen Chen</em>, and Lili Qiu </div> <div class="periodical"> <em>In Proceedings of the USENIX Symposium on Operating Systems Design and Implementation</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/2024_osdi_parrot.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The rise of large language models (LLMs) has enabled LLM-based applications (a.k.a. AI agents or co-pilots), a new software paradigm that combines the strength of LLM and conventional software. Diverse LLM applications from different tenants could design complex workflows using multiple LLM requests to accomplish one task. However, they have to use the over-simplified request-level API provided by today’s public LLM services, losing essential application-level information. Public LLM services have to blindly optimize individual LLM requests, leading to sub-optimal end-to-end performance of LLM applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">INFOCOM</abbr> </div> <div id="liu2024dpbalance" class="col-sm-8"> <div class="title">DPBalance: Efficient and Fair Privacy Budget Scheduling for Federated Learning as a Service</div> <div class="author"> Yu Liu, Zibo Wang, Yifei Zhu, and <em>Chen Chen</em> </div> <div class="periodical"> <em>In Proceedings of the IEEE Conference on Computer Communications</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/2024_infocom_dpbalance.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">HPCA</abbr> </div> <div id="zhang2024optimizing" class="col-sm-8"> <div class="title">An Optimizing Framework on MLIR for Efficient FPGA-based Accelerator Generation</div> <div class="author"> Weichuang Zhang, Jieru Zhao, Guan Shen, Quan Chen, <em>Chen Chen</em>, and Minyi Guo </div> <div class="periodical"> <em>In Proceedings of the IEEE International Symposium on High-Performance Computer Architecture</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button"><img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> Distinguished Artifact Award Runner-up</a> <a href="/assets/pdf/2024_hpca_pom.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>The <a href="https://www.hpca-conf.org/2024/" rel="external nofollow noopener" target="_blank">paper with the best artifact</a> in HPCA 2024 (2 out of 75 accepted papers and 410 submissions)</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ASPLOS</abbr> </div> <div id="li2023dataflower" class="col-sm-8"> <div class="title">DataFlower: Exploiting the Data-flow Paradigm for Serverless Workflow Orchestration</div> <div class="author"> Zijun Li, Chuhao Xu, Quan Chen, Jieru Zhao, <em>Chen Chen</em>, and Minyi Guo </div> <div class="periodical"> <em>In Proceedings of the ACM International Conference on Architectural Support for Programming Languages and Operating Systems</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/2024_asplos_dataflower.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=LSZQSruhxkc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICCD</abbr> </div> <div id="wang2023stag" class="col-sm-8"> <div class="title">STAG: Enabling Low Latency and Low Staleness of GNN-based Services with Dynamic Graphs</div> <div class="author"> Jiawen Wang, Quan Chen, Deze Zeng, Zhuo Song, <em>Chen Chen</em>, and Minyi Guo </div> <div class="periodical"> <em>In Proceedings of the IEEE International Conference on Computer Design</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/2023_iccd_stag.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">TCC</abbr> </div> <div id="chen_accelerating_2023" class="col-sm-8"> <div class="title">Accelerating Distributed Learning in Non-Dedicated Environments</div> <div class="author"> <em>Chen Chen</em>, Qizhen Weng, Wei Wang, Baochun Li, and Bo Li </div> <div class="periodical"> <em>IEEE Transactions on Cloud Computing</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/2023_tcc_lbbsp.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Machine learning (ML) models are increasingly trained with distributed workers possessing heterogeneous resources. In such scenarios, model training efﬁciency may be negatively affected by stragglers—workers that run much slower than others. Efﬁcient model training requires eliminating such stragglers, yet for modern ML workloads, existing load balancing strategies are inefﬁcient and even infeasible. In this paper, we propose a novel strategy, called semi-dynamic load balancing, to eliminate stragglers of distributed ML workloads. The key insight is that ML workers shall be load-balanced at iteration boundaries, being non-intrusive to intra-iteration execution. Based on it we further develop LB-BSP, an integrated worker coordination mechanism that adapts workers’ load to their instantaneous processing capabilities—by right-sizing the sample batches at the synchronization barriers. We have designed distinct load tuning algorithms for ML in CPU clusters, in GPU clusters as well as in federated learning setups, based on their respective characteristics. LB-BSP has been implemented as a Python module for ML frameworks like TensorFlow and PyTorch. Our EC2 deployment conﬁrms that LB-BSP is practical, effective and light-weight, and is able to accelerating distributed training by up to 54%.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICPP</abbr> </div> <div id="yu2023asfl" class="col-sm-8"> <div class="title">Asfl: Adaptive Semi-asynchronous Federated Learning for Balancing Model Accuracy and Total Latency in Mobile Edge Networks</div> <div class="author"> Jieling Yu, Ruiting Zhou, <em>Chen Chen</em>, Bo Li, and Fang Dong </div> <div class="periodical"> <em>In Proceedings of the ACM International Conference on Parallel Processing</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/2023_icpp_asfl.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">TPDS</abbr> </div> <div id="chen_synchronize_2023" class="col-sm-8"> <div class="title">Synchronize Only the Immature Parameters: Communication-Efficient Federated Learning By Freezing Parameters Adaptively</div> <div class="author"> <em>Chen Chen</em>, Hong Xu, Wei Wang, Baochun Li, Bo Li, Li Chen, and Gong Zhang </div> <div class="periodical"> <em>IEEE Transactions on Parallel and Distributed Systems</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/2023_tpds_apf.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Federated learning allows edge devices to collaboratively train a global model without sharing their local private data. Yet, with limited network bandwidth at the edge, communication often becomes a severe bottleneck. In this paper, we ﬁnd that it is unnecessary to always synchronize the full model in the entire training process, because many parameters already become mature (i.e., stable) prior to model convergence, and can thus be excluded from later synchronizations. This allows us to reduce the communication overhead without compromising the model accuracy. However, challenges are that the local parameters excluded from global synchronization may diverge on different clients, and meanwhile some parameters may stabilize only temporally. To address these challenges, we propose a novel scheme called Adaptive Parameter Freezing (APF), which ﬁxes (freezes) the non-synchronized stable parameters in intermittent periods. Speciﬁcally, the freezing periods are tentatively adjusted in an additively-increase and multiplicativelydecrease manner—depending on whether the previously-frozen parameters remain stable in subsequent iterations. We also extend APF into APF# and APF++, which freeze parameters in a more aggressive manner to achieve larger performance beneﬁt for large complex models. We implemented APF and its variants as Python modules with PyTorch, and extensive experiments show that APF can reduce data transfer amount by over 60%.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">JSAC</abbr> </div> <div id="chen_gift_2023" class="col-sm-8"> <div class="title">GIFT: Towards Accurate and Efficient Federated Learning with Gradient-Instructed Frequency Tuning</div> <div class="author"> <em>Chen Chen</em>, Hong Xu, Baochun Li, Bo Li, Li Chen, and Gong Zhang </div> <div class="periodical"> <em>IEEE Journal on Selected Areas in Communications (special issue on Communication-Efficient Distributed Learning over Networks)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/2023_jsac_gift.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Federated Learning (FL) enables distributed clients to collectively train a global model without revealing their private data, and for efficiency clients synchronize their gradients periodically. However, this can lead to the inaccuracy in model convergence due to inconsistent data distributions among clients. In this work, we find that there is a strong correlation between FL accuracy loss and the synchronization frequency, and seek to fine tune the synchronization frequency at training runtime to make FL accurate and also efficient. Specifically, aware that under the FL privacy requirement only gradients can be utilized for making frequency tuning decisions, we propose a novel metric called gradient consistency, which can effectively reflect the training status despite the instability of realistic FL scenarios. We further devise a feedback-driven algorithm called GradientInstructed Frequency Tuning (GIFT), which adaptively increases or decreases the synchronization frequency based on the gradient consistency metric. We have implemented GIFT in PyTorch, and large-scale evaluations show that it can improve FL accuracy by up to 10.7% with a time reduction of 58.1%.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">SoCC</abbr> </div> <div id="shi_characterizing_2022" class="col-sm-8"> <div class="title">Characterizing and orchestrating VM reservation in geo-distributed clouds to improve the resource efficiency</div> <div class="author"> Jiuchen Shi, Kaihua Fu, Quan Chen, Changpeng Yang, Pengfei Huang, Mosong Zhou, Jieru Zhao, <em>Chen Chen</em>, and Minyi Guo </div> <div class="periodical"> <em>In Proceedings of the ACM Symposium on Cloud Computing</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/2022_socc_ros.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Cloud providers often build a geo-distributed cloud from multiple datacenters in different geographic regions, to serve tenants at different locations. The tenants that run large scale applications often reserve resources based on their peak loads in the region close to the end users to handle the ever changing application load, wasting a large amount of resources. We therefore characterize the VM request patterns of the top tenants in our production public geo-distributed cloud, and open-source the VM request traces in four months from the top 20 tenants of our cloud. The characterization shows that the resource usage of large tenants has various temporal and spatial patterns on the dimensions of time series, regions, and VM types, and has the potential of peak shaving between different tenants to further reduce the resource reservation cost. Based on the findings, we propose a resource reservation and VM request scheduling scheme named ROS to minimize the resource reservation cost while satisfying the VM allocation requests. Our experiments show that ROS reduces the overall deployment cost by 75.4% and the reservation resources by 60.1%, compared to the tenant-specified reservation strategy.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICDCS</abbr> </div> <div id="chen_communication-efficient_2021" class="col-sm-8"> <div class="title">Communication-Efficient Federated Learning with Adaptive Parameter Freezing</div> <div class="author"> <em>Chen Chen</em>, Hong Xu, Wei Wang, Baochun Li, Bo Li, Li Chen, and Gong Zhang </div> <div class="periodical"> <em>In Proceedings of the IEEE International Conference on Distributed Computing Systems</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/2021_icdcs_apf.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=Zyuys2CWPwU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Federated learning allows edge devices to collaboratively train a global model by synchronizing their local updates without sharing private data. Yet, with limited network bandwidth at the edge, communication often becomes a severe bottleneck. In this paper, we ﬁnd that it is unnecessary to always synchronize the full model in the entire training process, because many parameters gradually stabilize prior to the ultimate model convergence, and can thus be excluded from being synchronized at an early stage. This allows us to reduce the communication overhead without compromising the model accuracy. However, challenges are that the local parameters excluded from global synchronization may diverge on different clients, and meanwhile some parameters may stabilize only temporally. To address these challenges, we propose a novel scheme called Adaptive Parameter Freezing (APF), which ﬁxes (freezes) the non-synchronized stable parameters in intermittent periods. Speciﬁcally, the freezing periods are tentatively adjusted in an additively-increase and multiplicatively-decrease manner, depending on if the previouslyfrozen parameters remain stable in subsequent iterations. We implemented APF as a Python module in PyTorch. Our extensive array of experimental results show that APF can reduce data transfer by over 60%.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IJCNN</abbr> </div> <div id="mo2021two" class="col-sm-8"> <div class="title">Two-dimensional learning rate decay: Towards accurate federated learning with non-iid data</div> <div class="author"> Kaiwei Mo, <em>Chen Chen</em>, Jiamin Li, Hong Xu, and Chun Jason Xue </div> <div class="periodical"> <em>In Proceedings of the International Joint Conference on Neural Networks</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/2021_ijcnn_2dlrd.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">SoCC</abbr> </div> <div id="chen_semi-dynamic_2020" class="col-sm-8"> <div class="title">Semi-Dynamic Load Balancing: Efficient Distributed Learning in Non-Dedicated Environments</div> <div class="author"> <em>Chen Chen</em>, Qizhen Weng, Wei Wang, Baochun Li, and Bo Li </div> <div class="periodical"> <em>In Proceedings of the ACM Symposium on Cloud Computing</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/2020_socc_lbbsp.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=TYJA_jfv1Cc&amp;t=1s" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Machine learning (ML) models are increasingly trained in clusters with non-dedicated workers possessing heterogeneous resources. In such scenarios, model training efficiency can be negatively affected by stragglers—workers that run much slower than others. Efficient model training requires eliminating such stragglers, yet for modern ML workloads, existing load balancing strategies are inefficient and even infeasible. In this paper, we propose a novel strategy called semi-dynamic load balancing to eliminate stragglers of distributed ML workloads. The key insight is that ML workers shall be load-balanced at iteration boundaries, being nonintrusive to intra-iteration execution. We develop LB-BSP based on such an insight, which is an integrated worker coordination mechanism that adapts workers’ load to their instantaneous processing capabilities by right-sizing the sample batches at the synchronization barriers. We have customdesigned the batch sizing algorithm respectively for CPU and GPU clusters based on their own characteristics. LB-BSP has been implemented as a Python module for ML frameworks like TensorFlow and PyTorch. Our EC2 deployment confirms that LB-BSP is practical, effective and light-weight, and is able to accelerating distributed training by up to 54%.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">SC</abbr> </div> <div id="wang2020metis" class="col-sm-8"> <div class="title">Metis: Learning to schedule long-running applications in shared container clusters at scale</div> <div class="author"> Luping Wang, Qizhen Weng, Wei Wang, <em>Chen Chen</em>, and Bo Li </div> <div class="periodical"> <em>In Proceedings of the ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/2020_sc_metis.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">INFOCOM</abbr> </div> <div id="chen2019round" class="col-sm-8"> <div class="title">Round-robin synchronization: Mitigating communication bottlenecks in parameter servers</div> <div class="author"> <em>Chen Chen</em>, Wei Wang, and Bo Li </div> <div class="periodical"> <em>In Proceedings of the IEEE Conference on Computer Communications</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button"><img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> Best In-Session Presentation Award</a> <a href="/assets/pdf/2019_infocom_rrsp.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>The best presentation for the session CLOUD COMPUTING 2</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">SoCC Poster</abbr> </div> <div id="chen2018fast" class="col-sm-8"> <div class="title">Fast distributed deep learning via worker-adaptive batch sizing</div> <div class="author"> <em>Chen Chen</em>, Qizhen Weng, Wei Wang, Baochun Li, and Bo Li </div> <div class="periodical"> <em>In Proceedings of the ACM symposium on cloud computing</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/2018_socc_lbbsp.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">INFOCOM</abbr> </div> <div id="chen_performance-aware_2018" class="col-sm-8"> <div class="title">Performance-Aware Fair Scheduling: Exploiting Demand Elasticity of Data Analytics Jobs</div> <div class="author"> <em>Chen Chen</em>, Wei Wang, and Bo Li </div> <div class="periodical"> <em>In Proceedings of the IEEE Conference on Computer Communications</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/2018_infocom_paf.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Efﬁcient resource management is of paramount importance in today’s production clusters. In this paper, we identify the demand elasticity of data-parallel jobs. Demand elasticity allows jobs to run with a signiﬁcantly less amount of resources than they ideally need, at the expense of only a modest performance penalty. Our EC2 experiment using popular Spark benchmark suites conﬁrms that running a job using 50% of demanded slots is sufﬁcient to achieve at least 75% of the ideal performance. We show that such an elasticity is an intrinsic property of data-parallel jobs and can be exploited to speed up average job completion. In this regard, we propose PerformanceAware Fair (PAF) scheduler to identify the demand elasticity and use it to improve the average job performance, while still attaining near-optimal isolation guarantee close to fair sharing. PAF starts with a fair allocation and iteratively adjusts it by transferring resources from one job to another, improving the performance of resource-taker without penalizing resource-giver by a noticeable amount. We implemented PAF in Spark and evaluated its effectiveness through both EC2 experiments and large-scale simulations. Evaluation results show that compared with fair allocation, PAF improves the average job performance by 13%, while penalizing resource-givers by no more than 1%.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICDCS</abbr> </div> <div id="chen_speculative_2017" class="col-sm-8"> <div class="title">Speculative Slot Reservation: Enforcing Service Isolation for Dependent Data-Parallel Computations</div> <div class="author"> <em>Chen Chen</em>, Wei Wang, and Bo Li </div> <div class="periodical"> <em>In Proceedings of the IEEE International Conference on Distributed Computing Systems</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/2017_icdcs_ssr.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Priority scheduling is a fundamental tool to provide service isolation for different jobs in shared clusters. Ideally, the performance of a high-priority job should not be dragged down by another with a lower priority. However, we show in this paper that simply assigning a high priority provides no isolation for jobs with dependent computations. A job, even receiving the highest priority, may give up compute slots to another before proceeding to the downstream computation, which is because of barrier, i.e., that the downstream computation cannot start until all the upstream tasks have completed. Such an interruption of execution inevitably results in a signiﬁcant delay.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">INFOCOM</abbr> </div> <div id="chen_cluster_2017" class="col-sm-8"> <div class="title">Cluster fair queueing: Speeding up data-parallel jobs with delay guarantees</div> <div class="author"> <em>Chen Chen</em>, Wei Wang, Shengkai Zhang, and Bo Li </div> <div class="periodical"> <em>In Proceedings of the IEEE Conference on Computer Communications</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button"><img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> Best In-Session Presentation Award</a> <a href="/assets/pdf/2017_infocom_cfq.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>The best presentation for the session CLOUD COMPUTING 1</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICC</abbr> </div> <div id="chen_software-defined_2016" class="col-sm-8"> <div class="title">Software-defined inter-domain routing revisited</div> <div class="author"> <em>Chen Chen</em>, Bo Li, Dong Lin, and Baochun Li </div> <div class="periodical"> <em>In Proceedings of the IEEE International Conference on Communications</em>, 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/2016_icc_sdn.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>With software-deﬁned networking (SDN), the control plane is fully decoupled from the data plane, which has been shown to improve routing performance and reduce route convergence time in the context of intra-domain routing. The applicability of software-deﬁned networking to inter-domain routing, however, has not been fully explored. In this work, we ﬁrst propose a mathematical model that attempts to quantify the BGP convergence time in an inter-domain routing environment, by simplifying the complex BGP convergence process. Based on our model and some practical observations, we ﬁrst investigate how software-deﬁned networking may help speed up interdomain routing, and then present a greedy algorithm that selects Autonomous Systems (ASes) for incremental SDN deployment to minimize the BGP convergence time. Our simulation results based on a real-world Internet topology have demonstrated the effectiveness of our proposed algorithm.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Chen Chen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: March 02, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>