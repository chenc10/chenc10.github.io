---
---

@string{aps = {American Physical Society,}}

@article{jiayi2025mitigating,
  abbr={ToN},
	title = {Mitigating Server-side Communication Bottlenecks in Distributed Learning with Round-Robin Participant Coordination},
	journal = {IEEE Transactions on Networking},
	author = {Zhang, Jiayi and Chen, Chen and Gan, Zuo and Wang, Wei and Li, Bo and Guo, Minyi},
	year = {2025},
}

@article{luo2025castor,
  abbr={TCC},
	title = {Castor: Optimizing Deep Learning Job Scheduling in Multi-Tenant GPU Clusters via Intelligent Colocation},
	journal = {IEEE Transactions on Cloud Computing},
	author = {Luo, Yizhou and Lai, Jiaxin and Shi, Shaohuai and Chen, Chen and Qi, Shuhan and Zhang, Jiajia and Wang, Qiang},
	year = {2025},
  pdf={2025_tcc_castor.pdf}
}

@inproceedings{wang2025semantic,
  abbr={CloudCom},
  title={SemanticPrefetcher: Accelerate Data Lake Access with Semantics-Aware File Prefetching},
  author={Wang, Tianze and Wang, Guanjie and Yang, Mingyan and Luo, Manqi and Zou, Mingchuan and Chen, Chen and Guo, Minyi},
  award={Awarded the excellent paper (1/3) of IEEE CloudCom},
  award_name={:trophy: Excellent Paper Award},
  booktitle={The 16th IEEE International Conference on Cloud Computing Technology and Science},
  year={2025}
}

@article{wang2025efficient,
  abbr={Arxiv},
  title={Efficient Unified Caching for Accelerating Heterogeneous AI Workloads},
  author={Wang, Tianze and Liu, Yifei and Chen, Chen and Zuo, Pengfei and Zhang, Jiawei and Weng, Qizhen and Chen, Yin and Han, Zhenhua and Zhao, Jieru and Chen, Quan and Guo, Minyi},
  journal={arXiv preprint arXiv:2506.12370},
  pdf={2025_arxiv_igtcache.pdf},
  year={2025}
}

@article{liu2025efficient,
  abbr={Arxiv},
  title={Efficient Serving of LLM Applications with Probabilistic Demand Modeling},
  author={Liu, Yifei and Gan, Zuo and Gan, Zhenghao and Wang, Weiye and Chen, Chen and Shan, Yizhou and Chen, Xusheng and Han, Zhenhua and Zhu, Yifei and Sun, Shixuan and others},
  journal={arXiv preprint arXiv:2506.14851},
  pdf={2025_arxiv_hermes.pdf},
  year={2025}
}

@inproceedings{di2025retrieval,
  abbr={NeurIPS},
  title={RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval},
  author={Liu, Di and Chen, Meng and Lu, Baotong and Jiang, Huiqiang and Han, Zhenhua and Zhang, Qianxi and Chen, Qi and Zhang, Chengruidong and Ding, Bailu and Zhang, Kai and Chen, Chen and Yang, Fan and Yang, Yuqing and Qiu, Lili},
  booktitle={The Thirty-Ninth Annual Conference on Neural Information Processing Systems},
  pdf={2025_neurips_retrievalattention.pdf},
  year={2025},
}

@article{liu2025ares,
  abbr={TACO},
	title = {Ares: Fair and Efficient Scheduling of Deep Learning Jobs with Elastic Fair Queuing},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Liu, Yifei and Chen, Chen and Wang, Qiang and Feng, Yu and Cui, Weihao and Chen, Quan and Guo, Minyi},
	year = {2025},
  pdf={2025_taco_ares.pdf}
}

@article{zhu2025trident,
  abbr={TSC},
	title = {Trident: A Provider-Oriented Resource Management Framework for Serverless Computing Platforms},
	journal = {IEEE Transactions on Services Computing},
	author = {Zhu, Botao and Zhu, Yifei and Chen, Chen and Kong, Linghe},
  pdf={2025_tsc_trident.pdf},
	year = {2025},
}

@article{hao2025rapidstore,
  abbr={VLDB},
  title={RapidStore: An Efficient Dynamic Graph Storage System for Concurrent Queries},
  author={Hao, Chiyu and Su, Jixian and Sun, Shixuan and Zhang, Hao and Gao, Sen and Zhao, Jianwen and Zhang, Chenyi and Zhao, Jieru and Chen, Chen and Guo, Minyi},
  journal={International Conference on Very Large Data Bases},
  pdf={2025_vldb_rapidstore.pdf},
  year={2025}
}

@article{zhao2025edas,
  abbr={TACO},
  title={EDAS: Enabling Fast Data Loading for GPU Serverless Computing},
  author={Zhao, Han and Cui, Weihao and Chen, Quan and Li, Zijun and Han, Zhenhua and Wang, Nan and Feng, Yu and Zhao, Jieru and Chen, Chen and Leng, Jingwen and others},
  journal={ACM Transactions on Architecture and Code Optimization},
  year={2025},
  publisher={ACM New York, NY}
}

@inproceedings{zhu2025llmsched,
  abbr={ICDCS},
	title = {LLMSched: Uncertainty-Aware Workload Scheduling for Compound LLM Applications},
	booktitle = {IEEE International Conference on Distributed Computing Systems},
	author = {Zhu, Botao and Chen, Chen and Fan, Xiaoyi and Zhu, Yifei},
  pdf={2025_icdcs_llmsched.pdf},
	year = {2025},
}

@inproceedings{yu2025fedsu,
  abbr={ICDCS},
	title = {FedSU: Communication-efficient Federated Learning with Speculative Updating},
	booktitle = {IEEE International Conference on Distributed Computing Systems},
	author = {Yu, Wei and Chen, Chen and Li, Qinbin and Zhao, Jieru and Sun, Shixuan and Li, Bo and Guo, Minyi},
	year = {2025},
  pdf={2025_icdcs_fedsu.pdf}
}

@inproceedings{feng2025lumina,
  abbr={ISCA},
	title = {Lumina: Real-Time Neural Rendering by Exploiting Computational  Redundancy},
	booktitle = {International Symposium on Computer Architecture},
	author = {Feng, Yu and Lin, Weikai and Cheng, Yuge and Liu, Zihan and Leng, Jingwen and Guo, Minyi and Chen, Chen and Sun, Shixuan and Zhu, Yuhao}, 
  pdf={2025_isca_lumina.pdf},
	year = {2025}
}

@article{pengyu2025taming,
  abbr={TACO},
	title = {Taming Flexible Job Packing in Deep Learning Training Clusters},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Yang, Pengyu and Cui, Weihao and Xue, Chunyu and Zhao, Han and Chen, Chen and Chen, Quan and Yang, Jing and Guo, Minyi},
	year = {2025},
  pdf={2025_taco_gimbal.pdf}
}

@inproceedings{guangqiang2025reducing,
  abbr={IPDPS},
  title={Reducing the End-to-End Latency of DNN-based Recommendation Systems Deployed in GPU Pools},
  author={Luan, Guangqiang and Pang, Pu and Chen, Quan and Xu, Guoyao and Zhang, Chi and Zi, Yanyi and Yu, Yinghao and Yang, Guodong and Zhang, Liping and Chen, Chen and Guo, Minyi},
  booktitle={39th IEEE International Parallel and Distributed Processing Symposium},
  year={2025},
  pdf={2025_ipdps_zero.pdf}
}

@inproceedings{di2024retrieval,
  abbr={ENLSP},
  title={RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval},
  author={Liu, Di and Chen, Meng and Lu, Baotong and Jiang, Huiqiang and Han, Zhenhua and Zhang, Qianxi and Chen, Qi and Zhang, Chengruidong and Ding, Bailu and Zhang, Kai and Chen, Chen and Yang, Fan and Yang, Yuqing and Qiu, Lili},
  booktitle={the 4th NeurIPS Workshop on Efficient Natural Language and Speech Processing (Spotlight)},
  selected={true},
  award={Awarded the best paper by [ENLSP](https://neurips2024-enlsp.github.io/index.html)},
  award_name={:trophy: Best Paper Award},
  year={2024},
  pdf={2024_RetrievalAttention.pdf}
}

@inproceedings{lv2024fedca,
  abbr={ICPP},
  title={FedCA: Efficient Federated Learning with Client Autonomy},
  author={Lv, Na and Shen, Zhi and Chen, Chen and Jiang, Zhifeng and Zhang, Jiayi and Chen, Quan and Guo, Minyi},
  booktitle={Proceedings of the 53rd ACM International Conference on Parallel Processing},
  selected={true},
  year={2024},
  pdf={2024_icpp_fedca.pdf}
}

@inproceedings{zuo2024pas,
  abbr={IWQoS},
  title={PAS: Towards Accurate and Efficient Federated Learning with Parameter-Adaptive Synchronization},
  author={Gan, Zuo and Chen, Chen and Zhang, Jiayi and Zeng, Gaoxiong and Zhu, Yifei and Zhao, Jieru and Chen, Quan and Guo, Minyi},
  booktitle={Proceedings of the IEEE/ACM International Symposium on Quality of Service},
  pages={494--503},
  selected={true},
  year={2024},
  pdf={2024_iwqos_pas.pdf}
}

@inproceedings{zuo2024pas,
  abbr={IWQoS Poster},
  title={Towards Efficient Compound Large Language Model System Serving in the Wild},
  author={Zhu, Yifei and Zhu, Botao and Chen, Chen and Fan, Xiaoyi},
  booktitle={Proceedings of the IEEE/ACM International Symposium on Quality of Service},
  selected={true},
  award={[The best poster for IWQoS 2024](https://iwqos2024.ieee-iwqos.org/program/award)},
  award_name={:trophy: Best Poster Award},
  year={2024},
  pdf={2024_iwqos_pstcs.pdf}
}

@inproceedings{lin_parrot_2024,
  abbr={OSDI},
	title = {Parrot: {Efficient} {Serving} of {LLM}-based {Applications} with {Semantic} {Variable}},
	abstract = {The rise of large language models (LLMs) has enabled LLM-based applications (a.k.a. AI agents or co-pilots), a new software paradigm that combines the strength of LLM and conventional software. Diverse LLM applications from different tenants could design complex workflows using multiple LLM requests to accomplish one task. However, they have to use the over-simplified request-level API provided by today’s public LLM services, losing essential application-level information. Public LLM services have to blindly optimize individual LLM requests, leading to sub-optimal end-to-end performance of LLM applications.},
	language = {en},
	booktitle = {Proceedings of the USENIX Symposium on Operating Systems Design and Implementation},
	author = {Lin, Chaofan and Han, Zhenhua and Zhang, Chengruidong and Yang, Yuqing and Yang, Fan and Chen, Chen and Qiu, Lili},
	year = {2024},
  selected={true},
  pdf={2024_osdi_parrot.pdf}
}

@inproceedings{liu2024dpbalance,
  abbr={INFOCOM},
  title={DPBalance: Efficient and Fair Privacy Budget Scheduling for Federated Learning as a Service},
  booktitle={Proceedings of the IEEE Conference on Computer Communications},
  author={Liu, Yu and Wang, Zibo and Zhu, Yifei and Chen, Chen},
  year={2024},
  selected={true},
  pdf={2024_infocom_dpbalance.pdf}
}

@inproceedings{zhang2024optimizing,
  abbr={HPCA},
  title={An Optimizing Framework on MLIR for Efficient FPGA-based Accelerator Generation},
  author={Zhang, Weichuang and Zhao, Jieru and Shen, Guan and Chen, Quan and Chen, Chen and Guo, Minyi},
  booktitle={Proceedings of the IEEE International Symposium on High-Performance Computer Architecture},
  pages={75--90},
  year={2024},
  award={The [paper with the best artifact](https://www.hpca-conf.org/2024/) in HPCA 2024 (2 out of 75 accepted papers and 410 submissions)},
  award_name={:trophy: Distinguished Artifact Award Runner-up},
  selected={true},
  pdf={2024_hpca_pom.pdf}
}

@inproceedings{li2023dataflower,
  abbr={ASPLOS},
  title={DataFlower: Exploiting the Data-flow Paradigm for Serverless Workflow Orchestration},
  author={Li, Zijun and Xu, Chuhao and Chen, Quan and Zhao, Jieru and Chen, Chen and Guo, Minyi},
  booktitle={Proceedings of the ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  year={2024},
  selected={true},
  video={https://www.youtube.com/watch?v=LSZQSruhxkc},
  pdf={2024_asplos_dataflower.pdf}
}

@inproceedings{wang2023stag,
  abbr={ICCD},
  title={STAG: Enabling Low Latency and Low Staleness of GNN-based Services with Dynamic Graphs},
  author={Wang, Jiawen and Chen, Quan and Zeng, Deze and Song, Zhuo and Chen, Chen and Guo, Minyi},
  booktitle={Proceedings of the IEEE International Conference on Computer Design},
  pages={170--173},
  year={2023},
  pdf={2023_iccd_stag.pdf}
}

@article{chen_accelerating_2023,
  abbr={TCC},
	title = {Accelerating {Distributed} {Learning} in {Non}-{Dedicated} {Environments}},
	volume = {11},
	doi = {10.1109/TCC.2021.3102593},
	abstract = {Machine learning (ML) models are increasingly trained with distributed workers possessing heterogeneous resources. In such scenarios, model training efﬁciency may be negatively affected by stragglers—workers that run much slower than others. Efﬁcient model training requires eliminating such stragglers, yet for modern ML workloads, existing load balancing strategies are inefﬁcient and even infeasible. In this paper, we propose a novel strategy, called semi-dynamic load balancing, to eliminate stragglers of distributed ML workloads. The key insight is that ML workers shall be load-balanced at iteration boundaries, being non-intrusive to intra-iteration execution. Based on it we further develop LB-BSP, an integrated worker coordination mechanism that adapts workers’ load to their instantaneous processing capabilities—by right-sizing the sample batches at the synchronization barriers. We have designed distinct load tuning algorithms for ML in CPU clusters, in GPU clusters as well as in federated learning setups, based on their respective characteristics. LB-BSP has been implemented as a Python module for ML frameworks like TensorFlow and PyTorch. Our EC2 deployment conﬁrms that LB-BSP is practical, effective and light-weight, and is able to accelerating distributed training by up to 54\%.},
	language = {en},
	number = {1},
	urldate = {2024-09-01},
	journal = {IEEE Transactions on Cloud Computing},
	author = {Chen, Chen and Weng, Qizhen and Wang, Wei and Li, Baochun and Li, Bo},
	year = {2023},
	pages = {515--531},
  pdf={2023_tcc_lbbsp.pdf}
}

@inproceedings{yu2023asfl,
  abbr={ICPP},
  title={Asfl: Adaptive Semi-asynchronous Federated Learning for Balancing Model Accuracy and Total Latency in Mobile Edge Networks},
  author={Yu, Jieling and Zhou, Ruiting and Chen, Chen and Li, Bo and Dong, Fang},
  booktitle={Proceedings of the ACM International Conference on Parallel Processing},
  pages={443--451},
  year={2023},
  pdf={2023_icpp_asfl.pdf}
}

@article{chen_synchronize_2023,
  abbr={TPDS},
	title = {Synchronize {Only} the {Immature} {Parameters}: {Communication}-{Efficient} {Federated} {Learning} {By} {Freezing} {Parameters} {Adaptively}},
	copyright = {All rights reserved},
	issn = {1045-9219, 1558-2183, 2161-9883},
	shorttitle = {Synchronize {Only} the {Immature} {Parameters}},
	url = {https://ieeexplore.ieee.org/document/10036106/},
	doi = {10.1109/TPDS.2023.3241965},
	abstract = {Federated learning allows edge devices to collaboratively train a global model without sharing their local private data. Yet, with limited network bandwidth at the edge, communication often becomes a severe bottleneck. In this paper, we ﬁnd that it is unnecessary to always synchronize the full model in the entire training process, because many parameters already become mature (i.e., stable) prior to model convergence, and can thus be excluded from later synchronizations. This allows us to reduce the communication overhead without compromising the model accuracy. However, challenges are that the local parameters excluded from global synchronization may diverge on different clients, and meanwhile some parameters may stabilize only temporally. To address these challenges, we propose a novel scheme called Adaptive Parameter Freezing (APF), which ﬁxes (freezes) the non-synchronized stable parameters in intermittent periods. Speciﬁcally, the freezing periods are tentatively adjusted in an additively-increase and multiplicativelydecrease manner—depending on whether the previously-frozen parameters remain stable in subsequent iterations. We also extend APF into APF\# and APF++, which freeze parameters in a more aggressive manner to achieve larger performance beneﬁt for large complex models. We implemented APF and its variants as Python modules with PyTorch, and extensive experiments show that APF can reduce data transfer amount by over 60\%.},
	language = {en},
	urldate = {2023-12-22},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Chen, Chen and Xu, Hong and Wang, Wei and Li, Baochun and Li, Bo and Chen, Li and Zhang, Gong},
	year = {2023},
	pages = {1--18},
  selected={true},
  pdf={2023_tpds_apf.pdf}
}

@article{chen_gift_2023,
  abbr={JSAC},
	title = {{GIFT}: {Towards} {Accurate} and {Efficient} {Federated} {Learning} with {Gradient}-{Instructed} {Frequency} {Tuning}},
  volume={41},
  number={4},
  pages={902--914},
	copyright = {All rights reserved},
	abstract = {Federated Learning (FL) enables distributed clients to collectively train a global model without revealing their private data, and for efficiency clients synchronize their gradients periodically. However, this can lead to the inaccuracy in model convergence due to inconsistent data distributions among clients. In this work, we find that there is a strong correlation between FL accuracy loss and the synchronization frequency, and seek to fine tune the synchronization frequency at training runtime to make FL accurate and also efficient. Specifically, aware that under the FL privacy requirement only gradients can be utilized for making frequency tuning decisions, we propose a novel metric called gradient consistency, which can effectively reflect the training status despite the instability of realistic FL scenarios. We further devise a feedback-driven algorithm called GradientInstructed Frequency Tuning (GIFT), which adaptively increases or decreases the synchronization frequency based on the gradient consistency metric. We have implemented GIFT in PyTorch, and large-scale evaluations show that it can improve FL accuracy by up to 10.7\% with a time reduction of 58.1\%.},
	language = {en},
	journal = {IEEE Journal on Selected Areas in Communications (special issue on Communication-Efficient Distributed Learning over Networks)},
	author = {Chen, Chen and Xu, Hong and Li, Baochun and Li, Bo and Chen, Li and Zhang, Gong},
	year = {2023},
  selected={true},
  pdf={2023_jsac_gift.pdf}
}


@inproceedings{shi_characterizing_2022,
  abbr={SoCC},
	address = {San Francisco California},
	title = {Characterizing and orchestrating {VM} reservation in geo-distributed clouds to improve the resource efficiency},
	copyright = {All rights reserved},
	isbn = {978-1-4503-9414-7},
	url = {https://dl.acm.org/doi/10.1145/3542929.3563490},
	doi = {10.1145/3542929.3563490},
	abstract = {Cloud providers often build a geo-distributed cloud from multiple datacenters in different geographic regions, to serve tenants at different locations. The tenants that run large scale applications often reserve resources based on their peak loads in the region close to the end users to handle the ever changing application load, wasting a large amount of resources. We therefore characterize the VM request patterns of the top tenants in our production public geo-distributed cloud, and open-source the VM request traces in four months from the top 20 tenants of our cloud. The characterization shows that the resource usage of large tenants has various temporal and spatial patterns on the dimensions of time series, regions, and VM types, and has the potential of peak shaving between different tenants to further reduce the resource reservation cost. Based on the findings, we propose a resource reservation and VM request scheduling scheme named ROS to minimize the resource reservation cost while satisfying the VM allocation requests. Our experiments show that ROS reduces the overall deployment cost by 75.4\% and the reservation resources by 60.1\%, compared to the tenant-specified reservation strategy.},
	language = {en},
	urldate = {2022-11-09},
	booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
	author = {Shi, Jiuchen and Fu, Kaihua and Chen, Quan and Yang, Changpeng and Huang, Pengfei and Zhou, Mosong and Zhao, Jieru and Chen, Chen and Guo, Minyi},
	year = {2022},
	pages = {94--109},
  pdf={2022_socc_ros.pdf}
}


@inproceedings{chen_communication-efficient_2021,
  abbr={ICDCS},
	title = {Communication-{Efficient} {Federated} {Learning} with {Adaptive} {Parameter} {Freezing}},
	abstract = {Federated learning allows edge devices to collaboratively train a global model by synchronizing their local updates without sharing private data. Yet, with limited network bandwidth at the edge, communication often becomes a severe bottleneck. In this paper, we ﬁnd that it is unnecessary to always synchronize the full model in the entire training process, because many parameters gradually stabilize prior to the ultimate model convergence, and can thus be excluded from being synchronized at an early stage. This allows us to reduce the communication overhead without compromising the model accuracy. However, challenges are that the local parameters excluded from global synchronization may diverge on different clients, and meanwhile some parameters may stabilize only temporally. To address these challenges, we propose a novel scheme called Adaptive Parameter Freezing (APF), which ﬁxes (freezes) the non-synchronized stable parameters in intermittent periods. Speciﬁcally, the freezing periods are tentatively adjusted in an additively-increase and multiplicatively-decrease manner, depending on if the previouslyfrozen parameters remain stable in subsequent iterations. We implemented APF as a Python module in PyTorch. Our extensive array of experimental results show that APF can reduce data transfer by over 60\%.},
	language = {en},
	booktitle = {Proceedings of the IEEE International Conference on Distributed Computing Systems},
	author = {Chen, Chen and Xu, Hong and Wang, Wei and Li, Baochun and Li, Bo and Chen, Li and Zhang, Gong},
	year = {2021},
  video={https://www.youtube.com/watch?v=Zyuys2CWPwU},
  selected={true},
  pdf={2021_icdcs_apf.pdf}
}

@inproceedings{mo2021two,
  abbr={IJCNN},
  title={Two-dimensional learning rate decay: Towards accurate federated learning with non-iid data},
  author={Mo, Kaiwei and Chen, Chen and Li, Jiamin and Xu, Hong and Xue, Chun Jason},
  booktitle={Proceedings of the International Joint Conference on Neural Networks},
  year={2021},
  pdf={2021_ijcnn_2dlrd.pdf}
}

@inproceedings{chen_semi-dynamic_2020,
  abbr={SoCC},
	title = {Semi-{Dynamic} {Load} {Balancing}: {Efficient} {Distributed} {Learning} in {Non}-{Dedicated} {Environments}},
	copyright = {All rights reserved},
	abstract = {Machine learning (ML) models are increasingly trained in clusters with non-dedicated workers possessing heterogeneous resources. In such scenarios, model training efficiency can be negatively affected by stragglers—workers that run much slower than others. Efficient model training requires eliminating such stragglers, yet for modern ML workloads, existing load balancing strategies are inefficient and even infeasible. In this paper, we propose a novel strategy called semi-dynamic load balancing to eliminate stragglers of distributed ML workloads. The key insight is that ML workers shall be load-balanced at iteration boundaries, being nonintrusive to intra-iteration execution. We develop LB-BSP based on such an insight, which is an integrated worker coordination mechanism that adapts workers’ load to their instantaneous processing capabilities by right-sizing the sample batches at the synchronization barriers. We have customdesigned the batch sizing algorithm respectively for CPU and GPU clusters based on their own characteristics. LB-BSP has been implemented as a Python module for ML frameworks like TensorFlow and PyTorch. Our EC2 deployment confirms that LB-BSP is practical, effective and light-weight, and is able to accelerating distributed training by up to 54\%.},
	booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
  video={https://www.youtube.com/watch?v=TYJA_jfv1Cc&t=1s},
	author = {Chen, Chen and Weng, Qizhen and Wang, Wei and Li, Baochun and Li, Bo},
	year = {2020},
	pages = {16},
  selected={true},
  pdf={2020_socc_lbbsp.pdf}
}

@inproceedings{wang2020metis,
  abbr={SC},
  title={Metis: Learning to schedule long-running applications in shared container clusters at scale},
  author={Wang, Luping and Weng, Qizhen and Wang, Wei and Chen, Chen and Li, Bo},
  booktitle={Proceedings of the ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis},
  year={2020},
  selected={true},
  pdf={2020_sc_metis.pdf}
}

@inproceedings{chen2019round,
  abbr={INFOCOM},
  award={The best presentation for the session CLOUD COMPUTING 2},
  award_name={:trophy: Best In-Session Presentation Award},
  title={Round-robin synchronization: Mitigating communication bottlenecks in parameter servers},
  author={Chen, Chen and Wang, Wei and Li, Bo},
  booktitle={Proceedings of the IEEE Conference on Computer Communications},
  pages={532--540},
  year={2019},
  selected={true},
  pdf={2019_infocom_rrsp.pdf}
}

@inproceedings{chen2018fast,
  abbr={SoCC Poster},
  title={Fast distributed deep learning via worker-adaptive batch sizing},
  author={Chen, Chen and Weng, Qizhen and Wang, Wei and Li, Baochun and Li, Bo},
  booktitle={Proceedings of the ACM symposium on cloud computing},
  pages={521--521},
  year={2018},
  pdf={2018_socc_lbbsp.pdf}
}

@inproceedings{chen_performance-aware_2018,
  abbr={INFOCOM},
	address = {Honolulu, HI},
	title = {Performance-{Aware} {Fair} {Scheduling}: {Exploiting} {Demand} {Elasticity} of {Data} {Analytics} {Jobs}},
	copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)},
	isbn = {978-1-5386-4128-6},
	shorttitle = {Performance-{Aware} {Fair} {Scheduling}},
	url = {https://ieeexplore.ieee.org/document/8486026/},
	doi = {10.1109/INFOCOM.2018.8486026},
	abstract = {Efﬁcient resource management is of paramount importance in today’s production clusters. In this paper, we identify the demand elasticity of data-parallel jobs. Demand elasticity allows jobs to run with a signiﬁcantly less amount of resources than they ideally need, at the expense of only a modest performance penalty. Our EC2 experiment using popular Spark benchmark suites conﬁrms that running a job using 50\% of demanded slots is sufﬁcient to achieve at least 75\% of the ideal performance. We show that such an elasticity is an intrinsic property of data-parallel jobs and can be exploited to speed up average job completion. In this regard, we propose PerformanceAware Fair (PAF) scheduler to identify the demand elasticity and use it to improve the average job performance, while still attaining near-optimal isolation guarantee close to fair sharing. PAF starts with a fair allocation and iteratively adjusts it by transferring resources from one job to another, improving the performance of resource-taker without penalizing resource-giver by a noticeable amount. We implemented PAF in Spark and evaluated its effectiveness through both EC2 experiments and large-scale simulations. Evaluation results show that compared with fair allocation, PAF improves the average job performance by 13\%, while penalizing resource-givers by no more than 1\%.},
	language = {en},
	urldate = {2020-09-19},
	booktitle = {Proceedings of the IEEE Conference on Computer Communications},
	author = {Chen, Chen and Wang, Wei and Li, Bo},
	year = {2018},
	pages = {504--512},
  pdf={2018_infocom_paf.pdf}
}

@inproceedings{chen_speculative_2017,
  abbr={ICDCS},
	address = {Atlanta, GA, USA},
	title = {Speculative {Slot} {Reservation}: {Enforcing} {Service} {Isolation} for {Dependent} {Data}-{Parallel} {Computations}},
	copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)},
	isbn = {978-1-5386-1792-2},
	shorttitle = {Speculative {Slot} {Reservation}},
	url = {http://ieeexplore.ieee.org/document/7979999/},
	doi = {10.1109/ICDCS.2017.174},
	abstract = {Priority scheduling is a fundamental tool to provide service isolation for different jobs in shared clusters. Ideally, the performance of a high-priority job should not be dragged down by another with a lower priority. However, we show in this paper that simply assigning a high priority provides no isolation for jobs with dependent computations. A job, even receiving the highest priority, may give up compute slots to another before proceeding to the downstream computation, which is because of barrier, i.e., that the downstream computation cannot start until all the upstream tasks have completed. Such an interruption of execution inevitably results in a signiﬁcant delay.},
	language = {en},
	urldate = {2020-09-19},
	booktitle = {Proceedings of the IEEE International Conference on Distributed Computing Systems},
	author = {Chen, Chen and Wang, Wei and Li, Bo},
	year = {2017},
  pdf={2017_icdcs_ssr.pdf}
}

@inproceedings{chen_cluster_2017,
  abbr={INFOCOM},
	address = {Atlanta, GA, USA},
	title = {Cluster fair queueing: {Speeding} up data-parallel jobs with delay guarantees},
	copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)},
	isbn = {978-1-5090-5336-0},
	shorttitle = {Cluster fair queueing},
	url = {http://ieeexplore.ieee.org/document/8056948/},
	doi = {10.1109/INFOCOM.2017.8056948},
	language = {en},
	urldate = {2020-09-19},
	booktitle = {Proceedings of the IEEE Conference on Computer Communications},
	publisher = {IEEE},
	author = {Chen, Chen and Wang, Wei and Zhang, Shengkai and Li, Bo},
	year = {2017},
  pdf={2017_infocom_cfq.pdf},
  award={The best presentation for the session CLOUD COMPUTING 1},
  award_name={:trophy: Best In-Session Presentation Award},
	pages = {1--9}
}


@inproceedings{chen_software-defined_2016,
  abbr={ICC},
	address = {Kuala Lumpur, Malaysia},
	title = {Software-defined inter-domain routing revisited},
	copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)},
	isbn = {978-1-4799-6664-6},
	url = {http://ieeexplore.ieee.org/document/7511033/},
	doi = {10.1109/ICC.2016.7511033},
	abstract = {With software-deﬁned networking (SDN), the control plane is fully decoupled from the data plane, which has been shown to improve routing performance and reduce route convergence time in the context of intra-domain routing. The applicability of software-deﬁned networking to inter-domain routing, however, has not been fully explored. In this work, we ﬁrst propose a mathematical model that attempts to quantify the BGP convergence time in an inter-domain routing environment, by simplifying the complex BGP convergence process. Based on our model and some practical observations, we ﬁrst investigate how software-deﬁned networking may help speed up interdomain routing, and then present a greedy algorithm that selects Autonomous Systems (ASes) for incremental SDN deployment to minimize the BGP convergence time. Our simulation results based on a real-world Internet topology have demonstrated the effectiveness of our proposed algorithm.},
	language = {en},
	urldate = {2020-09-19},
	booktitle = {Proceedings of the IEEE International Conference on Communications},
	author = {Chen, Chen and Li, Bo and Lin, Dong and Li, Baochun},
	year = {2016},
	pages = {1--6},
  pdf={2016_icc_sdn.pdf}
}














